{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Project Report\n",
    "\n",
    "### Logic\n",
    "\n",
    "This dataset collection provides information that can be used to make informed decisions relating to a safe, entertaining and convenient stay at an Airbnb in any New York City borough.  By joining Airbnb, Data.gov and Yelp datasets, the end user is provided with information that they can use for analysis and decision making, relating to not only Airbnb prices, listings, and reviews, but area crime.  Information is also provided about area restaurants, entertainment and services that can also be used for analysis.  \n",
    "\n",
    "## The ETL Process\n",
    "\n",
    "### (E)xtract\n",
    "\n",
    "Data was extracted from three sources:\n",
    "\n",
    "Kaggle, Data.gov and Yelp\n",
    "\n",
    "Kaggle’s \"Airbnb Open Data in NYC\" dataset, was used, which included CSV files that provided listing summaries, neighborhood information, and reviews of New York City Airbnb’s.  Data.gov provided crime information in CSV format.  Yelp was scraped in order to collect information on restaurants, entertainment, and services in the same New York City boroughs that were used in the Airbnb and Data.gov crime datasets: Brooklyn, Bronx, Manhattan, Queens and Staten Island.\n",
    "\n",
    "### (T)ransform\n",
    "\n",
    "CSV files provided on Kaggle and Data.gov required cleaning and joining.  Dev tools, MySQL and Pandas were used for that process.  Table columns were dropped, renamed, or reformatted if necessary for ease of use.  CSV’s were filtered by borough for joining and to establish the relationship between tables.  Data scraped from Yelp was converted into a dataframe, and loaded into a CSV.  This data was also cleaned, joined reformatted, and filtered according to borough, which was used as the primary key for all datasets.  \n",
    "\n",
    "### (L)oad\n",
    "\n",
    "The relational database MySQL was used to load our data, but we found that process difficult to execute because the data import process was extremely slow.  Consequently, Pandas to was used to clean and load our data.\n",
    "\n",
    "### Production Database Final Tables\n",
    "\n",
    "The final data table was created through a series of joins on the primary key \"Borough\".  It includes information that would allow the end user to efficiently analyze Airbnb data and also have access to crime information on the borough that they chose all in the same place.  Tables used included information about nearby restaurants, entertainment and services in order to provide a detailed picture of the Airbnb location as it relates to safety, entertainment and convenience.  \n",
    "\n",
    "The final table joined by Borough includes the Borough itself, a Borough ID, Airbnb Average Price and Reviews, Total Crime, including Misdeameanors, Violations, and Felonies, Nightlife Venues, Restaurants, and Entertainment, as well as Average Reviews for Nightlife, Restaurants and Entertainment. \n",
    "\n",
    "SQL Alchemy was used to import the data into MySQL.  A chunksize of 5000 was used for ease of import.  We chose MySQL because we felt that tables could be easily manipulated in this database.   \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
